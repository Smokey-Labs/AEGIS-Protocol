# Experiments

This folder documents **small, intentional experiments** conducted to observe real-world behavior relevant to AEGIS design decisions.

These are not benchmarks.
They are not marketing artifacts.
They are not proofs of correctness.

They exist to record **evidence that informed judgment**.

---

## Purpose

AEGIS is built on the premise that many AI risks emerge *before* automation — at the level of advice, influence, validation, and social dynamics.

The experiments documented here are designed to:

- Observe authority drift in natural environments
- Identify where governance pressure actually appears
- Test whether restraint preserves decision ownership
- Validate or falsify assumptions with lived evidence

Each experiment is intentionally:
- Small in scope
- Low risk
- Non-performative
- Non-optimized for engagement or reach

---

## What These Experiments Are

Each experiment records:

- **Environment** (platform, audience, context)
- **Method** (what was done and what was deliberately avoided)
- **Observed outcomes** (what actually happened)
- **Key insights** (what changed or was confirmed)
- **Implications for AEGIS** (why this matters)

They are written *after the fact*, once outcomes are known.

---

## What These Experiments Are Not

These documents do **not**:

- Claim statistical significance
- Assert generalizability beyond their scope
- Prove AEGIS is “correct”
- Serve as justification for predetermined conclusions

They exist to show **how conclusions were reached**, not to defend them.

---

## Why This Matters

Most AI systems describe *what* they believe.
Few document *why* they believe it.

AEGIS treats evidence as first-class.

If a design decision cannot be traced back to:
- Observation
- Constraint
- Or failed intuition

It does not belong in the system.

These experiments are part of that traceability.

---

## Reading Guidance

These documents are best read as:
- Field notes
- Design telemetry
- Governance calibration points

Not as success stories.

Restraint is a feature.
